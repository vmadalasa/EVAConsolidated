# -*- coding: utf-8 -*-
"""Madhu_final1_ EVA S11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wjAW1YfsdvmMZjoJ3WerrgmiVbmr1Zx4
"""

import psutil
def get_size(bytes, suffix="B"):
    factor = 1024
    for unit in ["", "K", "M", "G", "T", "P"]:
        if bytes < factor:
            return f"{bytes:.2f}{unit}{suffix}"
        bytes /= factor
print("="*40, "Memory Information", "="*40)
svmem = psutil.virtual_memory()
print(f"Total: {get_size(svmem.total)}") ; print(f"Available: {get_size(svmem.available)}")
print(f"Used: {get_size(svmem.used)}") ; print(f"Percentage: {svmem.percent}%")

!pip install albumentations==0.4.5 --quiet

!nvidia-smi

device = 'cuda'
from S11_model import S11_Model

model = S11_Model().to(device)

from augmentation import CIFAR10_AlbumTrans
from data import CIFAR10DataLoader
from data_summary import model_summary, display

trans = CIFAR10_AlbumTrans()
data = CIFAR10DataLoader(trans, batch_size=512)
train_loader, test_loader = data.get_loaders()
display(train_loader, 64)

model_summary(model)

import torch
import torch.nn as nn
import torch.optim as optim
from find_lr import LRFinder
model = S11_Model().to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr= 1e-6, momentum= 0.9)
lr_finder = LRFinder(model, optimizer, criterion,device= 'cuda')
#lr_finder.range_test(train_loader, start_lr = 1e-7, end_lr=100, num_iter=10*len(train_loader), step_mode='exp',diverge_th=5,accumulation_steps=1) #Fast AI Approach
#lr_finder.range_test(train_loader, val_loader=test_loader, start_lr = 0.0001, end_lr=4, num_iter=500, step_mode="linear")
lr_finder.range_test(train_loader, start_lr = 1e-7, end_lr=10, num_iter=10*len(train_loader),step_mode="exp",diverge_th=5,accumulation_steps=1) #, step_mode="linear" #new
# Plot learning rate vs loss
lr_finder.plot()

# Reset graph
lr_finder.reset()

print('Least Lost  ', min(lr_finder.history['loss']))
best_lr = lr_finder.history['lr'][lr_finder.history['loss'].index(min(lr_finder.history['loss']))]
print('Best lr : ', best_lr)

import torch
import torch.nn as nn
from test import Test
from train import Train
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR #ReduceLROnPlateau
model = S11_Model().to(device)


#optimizer = optim.SGD(model.parameters(), lr=best_lr, momentum= 0.9)
#test = Test(model, device, test_loader)
#train = Train(model, device, train_loader, optimizer)
#scheduler = ReduceLROnPlateau(optimizer, mode= 'max', patience= 2)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.95, nesterov=True, weight_decay=1e-6)
scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,
                 max_lr=0.003,
                 total_steps=24,
                 steps_per_epoch=len(train_loader),
                 epochs=25,
                 pct_start=0.2,
                 cycle_momentum=True,
                 base_momentum=0.85,
                 max_momentum=0.95,
                 div_factor=100.,
                 final_div_factor=100,
                 anneal_strategy ='cos',
                 last_epoch=-1)


test = Test(model, device, test_loader)
train = Train(model, device, train_loader, optimizer)

lr_list = []
epochs = 24
print('='*20 + 'START' + '='*20)
for epoch in range(epochs):
  print("EPOCH:", epoch)
  #print(lr_list)
  train.train(epoch)
  lr_list.append(optimizer.param_groups[0]['lr'])
  test.test()
  scheduler.step() 
  #test.test()

from graphs import acc_loss, testvtrain, class_acc
acc_loss(train, test)
testvtrain(train, test)

from evascheduler import get_misclassified
misclassified = get_misclassified()

best_model = S11_Model().to(device)
best_model.load_state_dict(torch.load('/content/classifier.pt'))
best_model.eval()
#print('Best Model Loaded!')

import plot
from plot import mis, gen_cam, plot_pred_cam, plot_act_cam
mis(best_model, device, test_loader, 25)

import matplotlib.pyplot as plt
import numpy as np
lr = np.zeros(7)
it = range(7)
for i in range (7):
  if(i%2==1):
    lr[i] =1;

plt.xlabel('iterations')
plt.ylabel('lr')
plt.plot(it,lr)

